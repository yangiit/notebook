{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45254d81-900c-4808-8c1a-092ae5147796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:01:38.267217Z",
     "start_time": "2025-05-01T17:00:27.261665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1 with CUDA 1201 (you have 2.7.0+cu126)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.3: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060 Laptop GPU. Num GPUs = 1. Max memory: 6.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/g/model/Qwen/Qwen2-0.5B/ does not have a padding token! Will use pad_token = <|PAD_TOKEN|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # æ–‡æœ¬æœ€å¤§é•¿åº¦\n",
    "dtype = None # è‡ªåŠ¨å‘ç°æ•°æ®ç±»å‹\n",
    "load_in_4bit = True # æ˜¯å¦ä½¿ç”¨4bitåŠ è½½æ¨¡å‹\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒçš„FastLanguageModelæ¨¡å‹åŠå…¶å¯¹åº”çš„åˆ†è¯å™¨ï¼ˆtokenizerï¼‰\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name=\"unsloth/Meta-LLama-3.1-8b\",  # å¦‚æœéœ€è¦ä»Hugging Faceè¿œç¨‹ä¸‹è½½æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ³¨é‡Šæ‰çš„å‚æ•°\n",
    "    model_name=\"/mnt/g/model/Qwen/Qwen2-0.5B/\",  # æŒ‡å®šæœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹è·¯å¾„ï¼Œé¿å…é‡å¤ä¸‹è½½\n",
    "    max_seq_length=max_seq_length,  # è®¾ç½®æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    dtype=dtype,  # æŒ‡å®šæ¨¡å‹æƒé‡çš„æ•°æ®ç±»å‹ï¼ˆä¾‹å¦‚ï¼štorch.float16 æˆ– torch.bfloat16ï¼‰\n",
    "    load_in_4bit=load_in_4bit  # æ˜¯å¦ä»¥4ä½é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œä»¥å‡å°‘æ˜¾å­˜å ç”¨\n",
    ")\n",
    "\n",
    "# å¾—åˆ° model æ¨¡å‹å’Œ tokenizer è§£ç å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e688aca334806e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:01:38.272832Z",
     "start_time": "2025-05-01T17:01:38.268802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896, padding_idx=151646)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef92f8c2b40e2e",
   "metadata": {},
   "source": [
    "```\n",
    "Qwen2ForCausalLM(\n",
    "  # å®šä¹‰æ¨¡å‹çš„ä¸»ä½“éƒ¨åˆ†ï¼ŒåŒ…å«åµŒå…¥å±‚ã€å¤šå±‚è§£ç å™¨å±‚ã€å½’ä¸€åŒ–å±‚å’Œæœ€ç»ˆçš„çº¿æ€§å±‚\n",
    "  (model): Qwen2Model(\n",
    "    # å®šä¹‰åµŒå…¥å±‚ï¼Œå°†è¾“å…¥çš„è¯æ±‡ç´¢å¼•è½¬æ¢ä¸º2048ç»´çš„å‘é‡ï¼Œä½¿ç”¨151936ä½œä¸ºè¯æ±‡è¡¨å¤§å°ï¼Œpadding_idx=151643è¡¨ç¤ºå¡«å……è¯çš„ç´¢å¼•\n",
    "    (embed_tokens): Embedding(151936, 2048, padding_idx=151643)\n",
    "    # å®šä¹‰ä¸€ä¸ªåŒ…å«24ä¸ªQwen2DecoderLayerçš„æ¨¡å—åˆ—è¡¨ï¼Œæ¯ä¸ªè§£ç å™¨å±‚åŒ…å«è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€è¾“å…¥å½’ä¸€åŒ–å±‚å’Œåæ³¨æ„åŠ›å½’ä¸€åŒ–å±‚\n",
    "    (layers): ModuleList(\n",
    "      (0-23): 24 x Qwen2DecoderLayer(\n",
    "        # å®šä¹‰è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…å«æŸ¥è¯¢ã€é”®ã€å€¼çš„çº¿æ€§æŠ•å½±å’Œè¾“å‡ºçº¿æ€§æŠ•å½±ï¼Œä»¥åŠæ—‹è½¬ä½ç½®åµŒå…¥\n",
    "        (self_attn): Qwen2Attention(\n",
    "          # æŸ¥è¯¢çº¿æ€§æŠ•å½±ï¼Œå°†2048ç»´çš„è¾“å…¥è½¬æ¢ä¸º2048ç»´çš„è¾“å‡ºï¼Œå¹¶å¸¦æœ‰åç½®\n",
    "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
    "          # é”®çº¿æ€§æŠ•å½±ï¼Œå°†2048ç»´çš„è¾“å…¥è½¬æ¢ä¸º2048ç»´çš„è¾“å‡ºï¼Œå¹¶å¸¦æœ‰åç½®\n",
    "          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
    "          # å€¼çº¿æ€§æŠ•å½±ï¼Œå°†2048ç»´çš„è¾“å…¥è½¬æ¢ä¸º2048ç»´çš„è¾“å‡ºï¼Œå¹¶å¸¦æœ‰åç½®\n",
    "          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
    "          # è¾“å‡ºçº¿æ€§æŠ•å½±ï¼Œå°†2048ç»´çš„è¾“å…¥è½¬æ¢ä¸º2048ç»´çš„è¾“å‡ºï¼Œä¸å¸¦åç½®\n",
    "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
    "          # æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œç”¨äºåœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­å¼•å…¥ä½ç½®ä¿¡æ¯\n",
    "          (rotary_emb): Qwen2RotaryEmbedding()\n",
    "        )\n",
    "        # å®šä¹‰å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼ŒåŒ…å«é—¨æ§æŠ•å½±ã€ä¸ŠæŠ•å½±ã€ä¸‹æŠ•å½±å’Œæ¿€æ´»å‡½æ•°\n",
    "        (mlp): Qwen2MLP(\n",
    "          # é—¨æ§æŠ•å½±ï¼Œå°†2048ç»´çš„è¾“å…¥è½¬æ¢ä¸º5504ç»´çš„è¾“å‡ºï¼Œä¸å¸¦åç½®\n",
    "          (gate_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n",
    "          # ä¸ŠæŠ•å½±ï¼Œå°†2048ç»´çš„è¾“å…¥è½¬æ¢ä¸º5504ç»´çš„è¾“å‡ºï¼Œä¸å¸¦åç½®\n",
    "          (up_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n",
    "          # ä¸‹æŠ•å½±ï¼Œå°†5504ç»´çš„è¾“å…¥è½¬æ¢ä¸º2048ç»´çš„è¾“å‡ºï¼Œä¸å¸¦åç½®\n",
    "          (down_proj): Linear4bit(in_features=5504, out_features=2048, bias=False)\n",
    "          # æ¿€æ´»å‡½æ•°ï¼Œä½¿ç”¨SiLUï¼ˆSigmoid Linear Unitï¼‰\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        # è¾“å…¥å½’ä¸€åŒ–å±‚ï¼Œç”¨äºå¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œeps=1e-06æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°\n",
    "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "        # åæ³¨æ„åŠ›å½’ä¸€åŒ–å±‚ï¼Œç”¨äºå¯¹è‡ªæ³¨æ„åŠ›æœºåˆ¶åçš„è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œeps=1e-06æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°\n",
    "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "      )\n",
    "    )\n",
    "    # æœ€ç»ˆçš„å½’ä¸€åŒ–å±‚ï¼Œç”¨äºå¯¹æ•´ä¸ªæ¨¡å‹çš„è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œeps=1e-06æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°\n",
    "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "    # æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œç”¨äºåœ¨æ•´ä¸ªæ¨¡å‹ä¸­å¼•å…¥ä½ç½®ä¿¡æ¯\n",
    "    (rotary_emb): Qwen2RotaryEmbedding()\n",
    "  )\n",
    "  # å®šä¹‰è¯­è¨€æ¨¡å‹çš„å¤´ï¼ˆheadï¼‰ï¼Œå°†2048ç»´çš„éšè—çŠ¶æ€è½¬æ¢ä¸º151936ç»´çš„è¾“å‡ºï¼Œä¸å¸¦åç½®\n",
    "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d9ee4d73b824a",
   "metadata": {},
   "source": [
    "## Step2ï¼šä½¿ç”¨LoRAå¾®è°ƒæ¨¡å‹ï¼Œæ›´æ–°æ¨¡å‹1%-10%çš„å‚æ•°\n",
    "\n",
    "LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§ç”¨äºæ¨¡å‹å¾®è°ƒçš„ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒé€šè¿‡é™ä½æ¨¡å‹çš„å‚æ•°æ•°é‡æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨å¾®è°ƒæ¨¡å‹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨LoRAæ¥æ›´æ–°æ¨¡å‹çš„1%-10%çš„å‚æ•°ï¼Œä»è€Œå‡å°‘æ¨¡å‹çš„å‚æ•°æ•°é‡ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚\n",
    "\n",
    "LoRA:LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\n",
    "\n",
    "åŸç†ï¼šçŸ©é˜µAä½¿ç”¨é«˜æ–¯åˆå§‹åŒ–ï¼Œå…ˆé™ç»´ï¼ŒçŸ©é˜µBä½¿ç”¨å…¨é›¶åˆå§‹åŒ–ï¼Œç„¶åå‡ç»´ã€‚è¿™é‡Œçš„ç»´åº¦æ§åˆ¶å‚æ•°å°±æ˜¯çŸ©é˜µçš„ç§© **r**ï¼Œå®ƒé€šå¸¸æ˜¯ä¸€ä¸ªå¾ˆå°çš„å‚æ•° (1,6,8,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907f797326192ca2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:01:59.253714Z",
     "start_time": "2025-05-01T17:01:57.200475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.3 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨FastLanguageModelè·å–æ¨¡å‹\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,  # åŸå§‹çš„åŸºç¡€æ¨¡å‹\n",
    "    r=16,  # LoRAå¾®è°ƒçš„ç§©ï¼ˆrankï¼‰ï¼Œæ§åˆ¶å‚æ•°é‡å’Œè¡¨è¾¾èƒ½åŠ› å»ºè®®å€¼æ˜¯ 8 16,32,64,128,256,512,1024\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],  # éœ€è¦åº”ç”¨LoRAçš„æ¨¡å—åˆ—è¡¨ï¼Œå³éœ€è¦å¯¹è¿™äº›æ¨¡å—è¿›è¡Œå¾®è°ƒ\n",
    "    lora_alpha=16,  # LoRAçš„ç¼©æ”¾å› å­ï¼Œå½±å“è®­ç»ƒæ—¶æƒé‡çš„é‡è¦æ€§\n",
    "    lora_dropout=0,  # LoRAå±‚çš„dropoutæ¦‚ç‡ï¼Œè®¾ç½®ä¸º0è¡¨ç¤ºä¸ä½¿ç”¨dropout\n",
    "    bias=\"none\",  # æ˜¯å¦å¯¹biasè¿›è¡Œå¾®è°ƒï¼Œ\"none\"è¡¨ç¤ºä¸å¾®è°ƒ\n",
    "    use_gradient_checkpointing=\"unsloth\",  # æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ä»¥èŠ‚çœæ˜¾å­˜ï¼Œå…·ä½“å®ç°æ–¹å¼ç”±å‚æ•°å€¼å®šä¹‰\n",
    "    random_state=3407,  # éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
    "    use_rslora=False,  # æ˜¯å¦ä½¿ç”¨RS-LoRAæ–¹æ³•ï¼Œé»˜è®¤ä¸ºFalse\n",
    "    loftq_config=None  # LoFTQé…ç½®ï¼Œå¦‚æœéœ€è¦ä½¿ç”¨LoFTQé‡åŒ–æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ­¤å¤„ä¼ å…¥é…ç½®\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63412eb647c82d1",
   "metadata": {},
   "source": [
    "è¿è¡ŒStep2åä¼šç”ŸæˆLoRAç›¸å…³ç»“æ„\n",
    "\n",
    "æœ‰äº†è¿™äº›ç»“æ„ï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡†å¤‡æ•°æ®æ¥è¿›è¡Œå¾®è°ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43989fd13b89da94",
   "metadata": {},
   "source": [
    "## Step3: ä½¿ç”¨ alpaca æ ¼å¼å®šä¹‰æ•°æ®\n",
    "\n",
    "è®­ç»ƒæ•°æ®å¿…é¡»æ˜¯ jsonl æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå­—æ®µï¼š\n",
    "- instruction: è¾“å…¥çš„æç¤ºè¯­ï¼Œå³ç”¨æˆ·è¾“å…¥çš„æŒ‡ä»¤æˆ–é—®é¢˜\n",
    "- input: å¯é€‰çš„è¾“å…¥ï¼Œå¦‚æœå­˜åœ¨ï¼Œåˆ™è¡¨ç¤ºç”¨æˆ·è¾“å…¥çš„é™„åŠ ä¿¡æ¯\n",
    "- output: æ¨¡å‹çš„è¾“å‡ºç»“æœï¼Œå³æ ¹æ®è¾“å…¥çš„æŒ‡ä»¤æˆ–é—®é¢˜ç”Ÿæˆçš„ç­”æ¡ˆæˆ–å›å¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8056c33657a8a97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:04:45.872967Z",
     "start_time": "2025-05-01T17:04:45.811048Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 232 examples [00:00, 10029.98 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:00<00:00, 34581.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# EOS_TOKEN = \"<|endoftext|>\"\n",
    "EOS_TOKEN = tokenizer.eos_token # æˆªè‡³ç¬¦å·ï¼Œå¦‚æœæ²¡æœ‰çš„è¯ï¼Œæ¨¡å‹ä¼šä¸€ç›´ç”Ÿæˆè¾“å‡ºï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§é•¿åº¦\n",
    "def format_prompts_func(examples):\n",
    "    instruction = examples[\"instruction\"]  # è·å–æŒ‡ä»¤åˆ—è¡¨\n",
    "    inputs = examples[\"input\"]  # è·å–è¾“å…¥åˆ—è¡¨\n",
    "    output = examples[\"output\"]  # è·å–è¾“å‡ºåˆ—è¡¨\n",
    "    texts = []\n",
    "    for instruction, inputs, output in zip(instruction, inputs, output):\n",
    "        text = alpaca_prompt.format(instruction, inputs, output) + EOS_TOKEN  # æ ¼å¼åŒ–æç¤ºæ–‡æœ¬å¹¶æ·»åŠ ç»“æŸç¬¦\n",
    "        texts.append(text)  # å°†æ ¼å¼åŒ–åçš„æ–‡æœ¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "    return {\"text\": texts}  # è¿”å›åŒ…å«æ ¼å¼åŒ–æ–‡æœ¬çš„å­—å…¸\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('./zh_tranditional/', split='train')  # åŠ è½½è®­ç»ƒæ•°æ®é›†\n",
    "dataset = dataset.map(format_prompts_func, batched=True)  # åº”ç”¨æ ¼å¼åŒ–å‡½æ•°åˆ°æ•°æ®é›†ï¼Œæ‰¹é‡å¤„ç†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143242dcd1410f3",
   "metadata": {},
   "source": [
    "## Step4: ä½¿ç”¨SFTrainerè¿›è¡Œå¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b37dec31e69193",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:05:15.295467Z",
     "start_time": "2025-05-01T17:05:13.918052Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:00<00:00, 360.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\", #å¯ä»¥æ˜¯inputï¼Œoutputï¼Œtextï¼Œä¹Ÿå¯ä»¥æ˜¯è‡ªå®šä¹‰çš„åˆ—åï¼Œå…¶å®è¿™é‡Œçš„textå°±æ˜¯ä¸Šé¢format_prompts_funcæ„å»ºçš„å­—å…¸\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=5,\n",
    "        # num_train_epochs=1,\n",
    "        max_steps=20,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"./outputs\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e844a8a18811880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:05:23.266785Z",
     "start_time": "2025-05-01T17:05:23.261670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: NVIDIA GeForce RTX 3060 Laptop GPU. Max_memory = 6.0 GB.\n",
      "0.633 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 /1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU memory: {gpu_stats.name}. Max_memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7adfc42db043ba06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:05:32.635751Z",
     "start_time": "2025-05-01T17:05:25.195617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 232 | Num Epochs = 1 | Total steps = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
      " \"-____-\"     Trainable parameters = 8,798,208/5,000,000,000 (0.18% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:05, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.074300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_state = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7db974720f0f79",
   "metadata": {},
   "source": [
    "## Step5: æ¨ç†å’Œé¢„æµ‹\n",
    "\n",
    "è¿è¡Œå¾®è°ƒåçš„æ¨¡å‹ï¼šæ ¹æ®è¾“å…¥çš„instructionå’Œinputï¼Œæ¨¡å‹ä¼šç”Ÿæˆå¯¹åº”çš„outputã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786bd71d722cb637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:30:55.951587Z",
     "start_time": "2025-05-01T17:30:46.857390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     13,\n",
      "           9645,    264,   2033,    429,  34901,  44595,    279,   1681,    382,\n",
      "          14374,  29051,    510,  14880, 100345,  87752,  82025, 104004,  46944,\n",
      "          85106, 102104, 103936,   3407,  14374,   5571,    510, 104455,  20412,\n",
      "         106850,  99602, 104799, 105034, 105167,    271,  14374,   5949,    510]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nè¯·æ ¹æ®ä»¥ä¸‹æ–‡ç« æ„å»ºä¸€ä¸ªéœ€è¦å›ç­”çš„é—®é¢˜ã€‚\\n\\n### Input:\\näººå·¥æ™ºèƒ½æ˜¯å½“ä»Šç§‘æŠ€é¢†åŸŸçš„çƒ­é—¨è¯é¢˜\\n\\n### Response:\\n################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# æ ¼å¼åŒ–è¾“å…¥\n",
    "formatted_input = alpaca_prompt.format(\n",
    "    \"è¯·æ ¹æ®ä»¥ä¸‹æ–‡ç« æ„å»ºä¸€ä¸ªéœ€è¦å›ç­”çš„é—®é¢˜ã€‚\",  # instruction\n",
    "    \"äººå·¥æ™ºèƒ½æ˜¯å½“ä»Šç§‘æŠ€é¢†åŸŸçš„çƒ­é—¨è¯é¢˜\",  # input\n",
    "    \"\",  # output\n",
    ")\n",
    "\n",
    "# ç¼–ç è¾“å…¥\n",
    "inputs = tokenizer(\n",
    "    [formatted_input],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "print(inputs)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1737243bef09327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:28:56.983704Z",
     "start_time": "2025-05-01T17:27:43.917064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=50,  # æ§åˆ¶ç”Ÿæˆçš„æœ€å¤§é•¿åº¦\n",
    "        num_return_sequences=1  # è¿”å›çš„åºåˆ—æ•°é‡\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "941015243e6a465e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T17:29:11.899664Z",
     "start_time": "2025-05-01T17:29:11.894914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "è¯·æ ¹æ®ä»¥ä¸‹æ–‡ç« æ„å»ºä¸€ä¸ªéœ€è¦å›ç­”çš„é—®é¢˜ã€‚\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "####################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################//-#######################################-#################################### 1.11.11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae5bb334b01ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. åˆ é™¤æ¨¡å‹å¼•ç”¨\n",
    "del model\n",
    "\n",
    "# 2. å¼ºåˆ¶åƒåœ¾å›æ”¶\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 3. æ¸…ç©ºCUDAç¼“å­˜ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 4. å¯é€‰ï¼šåˆ é™¤tokenizerï¼ˆå¦‚æœéœ€è¦é‡Šæ”¾å…¨éƒ¨èµ„æºï¼‰\n",
    "del tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
